{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4995de87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ortools.graph.python import max_flow\n",
    "from ortools.sat.python import cp_model\n",
    "from ortools.linear_solver import pywraplp\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cff3208",
   "metadata": {},
   "source": [
    "SETUP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffcad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_data():\n",
    "    with open('/workspaces/rtewr/Project/input.txt', 'r') as f:\n",
    "        num_papers,num_reviewers,reviews_per_paper = map(int, f.readline().strip().split())\n",
    "        willing_reviewers = {}\n",
    "        for i in range(num_papers):\n",
    "            line = list(map(int, f.readline().strip().split()))\n",
    "            paper_id = i+1\n",
    "            reviewers = line[1:]\n",
    "            willing_reviewers[paper_id] = reviewers\n",
    "    return num_papers, num_reviewers, reviews_per_paper, willing_reviewers\n",
    "def reverse_dict(willing_reviewers):\n",
    "    willing_papers = {}\n",
    "    for paper, reviewers in willing_reviewers.items():\n",
    "        for reviewer in reviewers:\n",
    "            if reviewer not in willing_papers:\n",
    "                willing_papers[reviewer] = []\n",
    "            willing_papers[reviewer].append(paper)\n",
    "    return willing_papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ca8323",
   "metadata": {},
   "source": [
    "# Paper Reviewer Assignment: Constraint Programming Model with OR-Tools\n",
    "\n",
    "## Problem Modeling\n",
    "\n",
    "### 1. **Variables**\n",
    "- **Binary Assignment Variables**:  \n",
    "  `x[(paper, reviewer)]`: Binary variable where:\n",
    "  - `1` if reviewer `reviewer` is assigned to paper `paper`\n",
    "  - `0` otherwise  \n",
    "  **Domain**: Defined only for valid pairs `(paper, reviewer)` where `reviewer ∈ L(paper)`.\n",
    "\n",
    "- **Load Variables**:  \n",
    "  `loads[reviewer]`: Integer variable representing the total number of papers assigned to reviewer `reviewer`.  \n",
    "  **Domain**: `0 ≤ loads[reviewer] ≤ num_papers`.\n",
    "\n",
    "- **Max Load Variable**:  \n",
    "  `max_load`: Integer variable representing the maximum load across all reviewers (objective to minimize).  \n",
    "  **Domain**: `0 ≤ max_load ≤ num_papers`.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 2. **Constraints**\n",
    "1. **Paper Assignment Constraint**:  \n",
    "   Each paper must be reviewed by exactly `reviews_per_paper` reviewers:  \n",
    "   ```python\n",
    "   for paper in range(1, num_papers + 1):\n",
    "       model.Add(sum(x[(paper, reviewer)] for reviewer in willing_reviewers[paper]) == reviews_per_paper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5db3d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n",
      "Execution time: 1.0602 seconds\n"
     ]
    }
   ],
   "source": [
    "@time_execution\n",
    "def main()->None:\n",
    "    # Read input data\n",
    "    num_papers, num_reviewers, reviews_per_paper, willing_reviewers = input_data()  \n",
    "    # Create the model\n",
    "    model = cp_model.CpModel()\n",
    "    # Create binary variables for each paper-reviewer pair\n",
    "    x= {}\n",
    "    for paper in range(1, num_papers + 1):\n",
    "        for reviewer in willing_reviewers[paper]:\n",
    "            x[(paper, reviewer)] = model.NewBoolVar(f'x[{paper},{reviewer}]')\n",
    "\n",
    "    # Each paper must be reviewed by exactly reviews_per_paper reviewers\n",
    "    for paper in range(1, num_papers + 1):\n",
    "        model.Add(sum(x[(paper, reviewer)] for reviewer in willing_reviewers[paper]) == reviews_per_paper)\n",
    "    # Load for each reviewer\n",
    "    loads = {}\n",
    "    for reviewer in range(1, num_reviewers + 1):\n",
    "        loads[reviewer] = model.NewIntVar(0, num_papers, f'load[{reviewer}]')\n",
    "        model.Add(loads[reviewer] == sum(x[(paper, reviewer)] for paper in range(1, num_papers + 1) if (paper, reviewer) in x))\n",
    "    \n",
    "    # Add constraints max of loads is minium\n",
    "    max_load = model.NewIntVar(0, num_papers, 'max_load')\n",
    "    for reviewer in range(1, num_reviewers + 1):\n",
    "        model.Add(loads[reviewer] <= max_load)\n",
    "\n",
    "    # Objective: minimize the maximum load\n",
    "    model.Minimize(max_load)\n",
    "\n",
    "    # Solve the model\n",
    "    solver = cp_model.CpSolver()\n",
    "    status = solver.Solve(model)\n",
    "    # Print the solution\n",
    "    if status == cp_model.OPTIMAL or status == cp_model.FEASIBLE:\n",
    "        \"\"\"print(num_papers)\n",
    "        for paper in range(1, num_papers + 1):\n",
    "            print(reviews_per_paper, end=' ')\n",
    "            for reviewer in willing_reviewers[paper]:\n",
    "                if solver.Value(x[(paper, reviewer)]) == 1:\n",
    "                    print(reviewer, end=' ')\n",
    "            print()\"\"\"\n",
    "        print(solver.ObjectiveValue())\n",
    "    else:\n",
    "        print('No solution found.')\n",
    "    \"\"\"# Print statistics\n",
    "    print()\n",
    "    print('Statistics:')\n",
    "    print(f'  status   : {solver.StatusName(status)}')\n",
    "    print(f'  conflicts: {solver.NumConflicts()}')\n",
    "    print(f'  branches : {solver.NumBranches()}')\n",
    "    print(f'  wall time: {solver.WallTime()} ms')\n",
    "    print(f'  load     : {solver.ObjectiveValue()}')\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "31ff01fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_data(num_papers,num_reviewers,reviews_per_paper ,willing_reviewers,max_load):\n",
    "    \n",
    "    source = 0\n",
    "    sink = num_papers + num_reviewers + 1\n",
    "\n",
    "    start_nodes = []\n",
    "    end_nodes = []\n",
    "    capacities = []\n",
    "\n",
    "    # Arcs from source to papers\n",
    "    start_nodes += [source] * num_papers\n",
    "    end_nodes += [i for i in range(1, num_papers + 1)]\n",
    "    capacities += [reviews_per_paper] * num_papers\n",
    "\n",
    "    # Arcs from papers to reviewers (with correct offset)\n",
    "    for paper in range(1, num_papers + 1):\n",
    "        for reviewer in willing_reviewers[paper]:\n",
    "            start_nodes.append(paper)\n",
    "            end_nodes.append(reviewer + num_papers)  # Add offset here\n",
    "            capacities.append(1)\n",
    "\n",
    "    # Arcs from reviewers to sink\n",
    "    for reviewer in range(1, num_reviewers + 1):\n",
    "        start_nodes.append(reviewer + num_papers)\n",
    "        end_nodes.append(sink)\n",
    "        capacities.append(max_load)\n",
    "    return start_nodes, end_nodes, capacities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38b0db2",
   "metadata": {},
   "source": [
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "61497532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "Execution time: 0.0061 seconds\n"
     ]
    }
   ],
   "source": [
    "@time_execution\n",
    "def max_flow_method(): \n",
    "    # Instantiate a SimpleMaxFlow solver.\n",
    "    smf = max_flow.SimpleMaxFlow()\n",
    "\n",
    "    # Read input data\n",
    "    num_papers, num_reviewers, reviews_per_paper, willing_reviewers = input_data()\n",
    "\n",
    "    #Minimum capactices of max_load\n",
    "    if (num_papers*reviews_per_paper) % num_reviewers == 0:\n",
    "        low=(num_papers*reviews_per_paper) // num_reviewers\n",
    "    else:\n",
    "        low=(num_papers*reviews_per_paper) // num_reviewers + 1\n",
    "    \n",
    "    willing_papers = reverse_dict(willing_reviewers)\n",
    "    willing_papers=dict(sorted(willing_papers.items(), key=lambda item: len(item[1])))\n",
    "    high= max(len(papers) for papers in willing_papers.values()) if willing_papers else 0\n",
    "        \n",
    "    #Dirichlet's theorem\n",
    "    max_load= low\n",
    "\n",
    "    while max_load<=high:        \n",
    "        start_nodes, end_nodes, capacities = pre_processing_data(num_papers,num_reviewers,reviews_per_paper ,willing_reviewers,max_load)\n",
    "        #   note: we could have used add_arc_with_capacity(start, end, capacity)\n",
    "        all_arcs = smf.add_arcs_with_capacity(start_nodes, end_nodes, capacities)\n",
    "\n",
    "        # Find the maximum flow between node 0 and node 4.\n",
    "        status = smf.solve(0, num_papers + num_reviewers + 1)\n",
    "\n",
    "        if (status == smf.OPTIMAL) and smf.optimal_flow()== num_papers * reviews_per_paper:\n",
    "            # Print the solution\n",
    "            \"\"\"print(num_papers)\n",
    "            solution_flows = smf.flows(all_arcs)\n",
    "            arc_indices = {arc: i for i, arc in enumerate(zip(start_nodes, end_nodes))}\n",
    "       \n",
    "            for paper in range(1, num_papers + 1):\n",
    "                print(reviews_per_paper, end=' ')\n",
    "                assigned_reviewers = []\n",
    "                \n",
    "                # Check all arcs from this paper to reviewers\n",
    "                for reviewer in willing_reviewers[paper]:\n",
    "                    arc = (paper, reviewer + num_papers)\n",
    "                    if arc in arc_indices:\n",
    "                        flow_index = arc_indices[arc]\n",
    "                        if solution_flows[flow_index] == 1:\n",
    "                            assigned_reviewers.append(reviewer)\n",
    "                \n",
    "                # Print assigned reviewers\n",
    "                for rev in assigned_reviewers[:reviews_per_paper]:  # Ensure we don't exceed required reviews\n",
    "                    print(rev, end=' ')\n",
    "                print()\"\"\"\n",
    "            print(max_load) \n",
    "            break\n",
    "        else:\n",
    "            max_load += 1\n",
    "\n",
    "       \n",
    "        \n",
    "max_flow_method()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6a2a691d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "Execution time: 0.0050 seconds\n"
     ]
    }
   ],
   "source": [
    "def matching_papers(num_papers, num_reviewers, reviews_per_paper, willing_reviewers):\n",
    "    load = [0] * (num_reviewers + 1)\n",
    "    sorted_dict = dict(sorted(willing_reviewers.items(), key=lambda item: len(item[1])))\n",
    "    selected_reviewers = {}\n",
    "    for paper,reviewers in sorted_dict.items():\n",
    "        #Sort the reviewers by their current load\n",
    "        reviewers.sort(key=lambda x: load[x])\n",
    "        # Select the first K reviewers\n",
    "        selected_reviewers[paper] = reviewers[:reviews_per_paper]\n",
    "        # Update the load of the selected reviewers\n",
    "        for reviewer in selected_reviewers[paper]:\n",
    "            load[reviewer] += 1\n",
    "    # Find the maximum load\n",
    "    max_load = max(load[1:])\n",
    "    return max_load, selected_reviewers\n",
    "@time_execution\n",
    "def greedy_method(): \n",
    "    # Read input data\n",
    "    num_papers, num_reviewers, reviews_per_paper, willing_reviewers = input_data()\n",
    "    \n",
    "    # Call the matching function\n",
    "    max_load, selected_reviewers = matching_papers(num_papers, num_reviewers, reviews_per_paper, willing_reviewers)\n",
    "    \n",
    "    # Print the result\n",
    "    \"\"\"print(num_papers)\n",
    "    for paper in range(1, num_papers + 1):\n",
    "        print(reviews_per_paper, end=' ')\n",
    "        for reviewer in selected_reviewers.get(paper, []):\n",
    "            print(reviewer, end=' ')\n",
    "        print()\"\"\"\n",
    "    print(max_load)\n",
    "    \n",
    "greedy_method() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0e1ce781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max load: 30\n",
      "Execution time: 0.0071 seconds\n"
     ]
    }
   ],
   "source": [
    "def matching_papers_2(num_papers, num_reviewers, reviews_per_paper, willing_reviewers):\n",
    "    load = {i: 0 for i in range(1, num_reviewers + 1)}  # Fixed: Start from 1 instead of 0\n",
    "    sorted_dict = dict(sorted(willing_reviewers.items(), key=lambda item: len(item[1])))\n",
    "    selected_reviewers = {}\n",
    "    for paper,reviewers in sorted_dict.items():\n",
    "        #Sort the reviewers by their current load\n",
    "        reviewers.sort(key=lambda x: load[x])\n",
    "        # Select the first K reviewers\n",
    "        selected_reviewers[paper] = reviewers[:reviews_per_paper]\n",
    "        # Update the load of the selected reviewers\n",
    "        for reviewer in selected_reviewers[paper]:\n",
    "            load[reviewer] += 1\n",
    "    # Find the maximum load\n",
    "    max_load = max(load.values())\n",
    "    return max_load, selected_reviewers, load\n",
    "\n",
    "def local_search(num_papers, num_reviewers, reviews_per_paper, willing_papers, assigned_papers, current_load, depth=0):\n",
    "    if depth > 100:  # Limit recursion depth\n",
    "        return assigned_papers, current_load\n",
    "    \n",
    "    changed = False\n",
    "    sorted_load = dict(sorted(current_load.items(), key=lambda item: item[1]))\n",
    "    #get the max load\n",
    "    reviewer_of_max_load = max(current_load, key=current_load.get)\n",
    "    max_load = current_load[reviewer_of_max_load]\n",
    "    \n",
    "    for candidate in sorted_load.keys():\n",
    "        if sorted_load[candidate] < max_load-1:\n",
    "            # Ensure both reviewers exist in the dictionaries\n",
    "            if candidate not in willing_papers:\n",
    "                willing_papers[candidate] = []\n",
    "            if reviewer_of_max_load not in assigned_papers:\n",
    "                continue\n",
    "                \n",
    "            available_papers = list(set(willing_papers.get(candidate, [])) & set(assigned_papers[reviewer_of_max_load]))\n",
    "            if len(available_papers) > 0:\n",
    "                random_paper = random.choice(available_papers)\n",
    "                assigned_papers[candidate].append(random_paper)\n",
    "                assigned_papers[reviewer_of_max_load].remove(random_paper)\n",
    "                # Update the load\n",
    "                current_load[candidate] += 1\n",
    "                current_load[reviewer_of_max_load] -= 1\n",
    "                changed=True\n",
    "                break\n",
    "                \n",
    "    if changed:\n",
    "        return local_search(num_papers, num_reviewers, reviews_per_paper, willing_papers, assigned_papers, current_load, depth+1)\n",
    "\n",
    "    return assigned_papers, current_load\n",
    "@time_execution\n",
    "def local_search_method():\n",
    "    num_papers, num_reviewers, reviews_per_paper, willing_reviewers = input_data()\n",
    "    willing_papers = reverse_dict(willing_reviewers)\n",
    "\n",
    "    \n",
    "    max_load, selected_reviewers, current_load = matching_papers_2(num_papers, num_reviewers, reviews_per_paper, willing_reviewers)\n",
    "    \n",
    "    # Calculate assigned papers for each reviewer\n",
    "    assigned_papers = reverse_dict(selected_reviewers)\n",
    "    \n",
    "    # Update willing papers by removing already assigned papers\n",
    "    reviewer_willing_papers = {}\n",
    "    for i in range(1, num_reviewers+1):\n",
    "        if i in willing_papers:\n",
    "            reviewer_willing_papers[i] = list(set(willing_papers[i]) - set(assigned_papers.get(i, [])))\n",
    "    \n",
    "    # Perform local search\n",
    "    optimized_assignments, final_load = local_search(num_papers, num_reviewers, reviews_per_paper, reviewer_willing_papers, assigned_papers, current_load)\n",
    "    selected_reviewers=reverse_dict(optimized_assignments)\n",
    "    \"\"\"print(num_papers)\n",
    "    for paper, reviewers in selected_reviewers.items():\n",
    "        print(f\"{reviews_per_paper} {' '.join(map(str, reviewers))}\")\"\"\"\n",
    "    print(f\"Max load: {max(final_load.values())}\")\n",
    "\n",
    "local_search_method()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
